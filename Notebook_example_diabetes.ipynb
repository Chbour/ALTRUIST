{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tweepy==3.9.0 pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import torch\n",
    "import tweepy\n",
    "import swifter\n",
    "import pandas as pd\n",
    "from Scripts import Data2Cox\n",
    "from Scripts import DefThreshold\n",
    "from Scripts import MongodbInit\n",
    "from Scripts import DataCollection\n",
    "from Scripts import ApplyClassifier\n",
    "from Scripts import ChooseTransformer\n",
    "from Scripts import TimelineCollection\n",
    "from Scripts import TimelineSimilarities\n",
    "from Scripts import TimelinePreprocessing\n",
    "from Scripts import TrainPersonalClassifier\n",
    "import torch.nn.functional as F\n",
    "from lifelines import CoxPHFitter\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from transformers import TrainingArguments, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initMongo = MongodbInit.MongoInitialisation()\n",
    "db = initMongo.mongodb_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation and data collection based of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize connexion to Twitter using the tokens generated on Twitter API and filled in the config file\n",
    "initCollection = DataCollection.InitialisationTwitter()\n",
    "auth = initCollection.connection_to_twitter()\n",
    "\n",
    "#Load keywords filled in the config file\n",
    "keywords = initCollection.keywords\n",
    "\n",
    "#Connect to the API and collect tweets based on the list of keywords\n",
    "api = tweepy.API(wait_on_rate_limit=True)\n",
    "streamlistener = DataCollection.StreamListener(db, auth, api)\n",
    "streamer = tweepy.Stream(auth=auth, listener=streamlistener)\n",
    "time.sleep(2)\n",
    "print(\"Tracking: \" + str(keywords))\n",
    "while True: #this will keep collecting tweets while the cell is running, you'll have to stop it or set a threshold to collect a limited number of tweets\n",
    "    streamer.filter(track=keywords, stall_warnings = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train personal / own diabetes classifier using labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset with tweets you have manually labeled to identify users tweeting about their own experience\n",
    "dataset = pd.read_csv(\"Own_diabetes_manually_labeled_tweets.csv\")\n",
    "dataset = dataset.rename(columns={\"labels\":\"label\"}) #the column with the labels has to be called label for the next step\n",
    "initTrain = TrainPersonalClassifier.DataToTrain(TweetTokenizer)\n",
    "\n",
    "#Prepare encodings for training\n",
    "train_encodings, train_labels, val_encodings, val_labels, test_encodings, test_labels = initTrain.PrepareData(dataset)\n",
    "train_dataset = TrainPersonalClassifier.TweetDataSet(train_encodings, train_labels)\n",
    "val_dataset = TrainPersonalClassifier.TweetDataSet(val_encodings, val_labels)\n",
    "test_dataset = TrainPersonalClassifier.TweetDataSet(test_encodings, test_labels)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))\n",
    "\n",
    "#These arguments will be used, it is possible to modify them\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "#Pretrained HF model used that will be fine tuned\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "#Training\n",
    "modeltrainer = TrainPersonalClassifier.ModelTrainer()\n",
    "trainer = modeltrainer.trainer(training_args, model, train_dataset, val_dataset)\n",
    "trainer.train()\n",
    "eval_output = trainer.evaluate(test_dataset)\n",
    "\n",
    "#This will print the model performances\n",
    "print(eval_output)\n",
    "\n",
    "#Save the model\n",
    "trainer.save_model(\"Personal_experience_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply classifier to collected tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load collected tweets into a pandas dataframe\n",
    "tweets = pd.DataFrame(db.tweets_collection.find())\n",
    "\n",
    "# Initialize data object to preprocess tweets\n",
    "init_data = ApplyClassifier.Data(TweetTokenizer)\n",
    "\n",
    "# Determine device for model (GPU if available, else CPU)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device: {}\".format(device))\n",
    "\n",
    "# Load tokenizer and model for sequence classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Personal_experience_model\").to(device)\n",
    "\n",
    "# If model was fine-tuned on english tweets only, filter tweets to only include English tweets\n",
    "tweets = tweets[tweets[\"lang\"] == \"en\"]\n",
    "print(tweets.shape)\n",
    "\n",
    "# Encode tweet text using the tokenizer, truncating and padding the sequences as necessary\n",
    "# => allows to feed batches of sequences \n",
    "tweets_encodings = tokenizer(tweets.text.map(init_data.normalizeTweet).values.tolist(), truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Create a dataset object from the encoded tweets\n",
    "tweetDataSet = ApplyClassifier.TweetDataSet(tweets_encodings)\n",
    "\n",
    "# Set model to evaluation mode and create a dataloader for the tweet dataset\n",
    "model.eval()\n",
    "tweetsLoader = DataLoader(tweetDataSet, batch_size=32)\n",
    "print(\"len tweetsLoader: {}\".format(len(tweetsLoader)))\n",
    "\n",
    "# Predict the label for each tweet using the model and append the predicted labels to a pandas series\n",
    "predicted = pd.Series()\n",
    "for (i, batch) in enumerate(tweetsLoader): \n",
    "    if i % 2000 == 0 : print(i)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    proba = F.softmax(outputs[0]).detach().cpu().numpy()  # get probabilities from output\n",
    "    predicted_labels = pd.DataFrame(proba).apply(tweetDataSet.proba_to_category, axis=1) # get predicted class (highest proba)\n",
    "    predicted = predicted.append(predicted_labels, ignore_index=True)\n",
    "\n",
    "# Print the counts of each label and filter the tweets to only include those where the predicted label is 1 (personal experience)\n",
    "print(\"predicted: {}\".format(predicted.shape))\n",
    "print(predicted.value_counts())\n",
    "tweets[\"personal\"] = predicted.values\n",
    "tweets_personal = tweets[tweets[\"personal\"] == 1] #Keep only the ones where users talk about their own diabetes\n",
    "print(\"personal tweets: {}\".format(tweets_personal.shape))\n",
    "\n",
    "# Drop unnecessary columns and reorder columns for output\n",
    "tweets_personal = tweets_personal.drop([\"_id\"], axis=1)\n",
    "tweets_personal = tweets_personal[[\"id\",\"user\"]]\n",
    "\n",
    "# Write personal tweets to a CSV file\n",
    "#tweets_personal.to_csv(\"personal_tweets.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timelines collection of identified users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load class to collect timelines\n",
    "api_timeline = tweepy.API(auth)\n",
    "timelinecol = TimelineCollection.TimelineCollection(db, api_timeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check ids that still exist\n",
    "list_ids = [tweet[\"user\"][\"id\"] for index, tweet in tweets_personal.iterrows()]\n",
    "list_existingids = [userid for userid in list_ids if timelinecol.exists(userid)]\n",
    "print(len(list_existingids))\n",
    "\n",
    "#Collect user ids still active/existing, timelines will be stored in the test_timeline collection in mongodb\n",
    "timelinecol.collect_timelines(list_existingids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of users timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define keywords that need to be matched in tweets for preprocessing\n",
    "keywords_to_match = [\"t1d\", \"t2d\", \"insulin\", \"gbdoc\", \"dsma\",\"type 1\", \"type 2\", \"diabetic\", \"diabete\", \"blood glucose\", \"fingerprick\"]\n",
    "\n",
    "#Key/main concept\n",
    "dict_key_concept = {\"Diabetes\" : \"diabetes\"}\n",
    "\n",
    "# Initialize a TimelinePreprocessing object with the keyword-to-concept mapping and the list of keywords to match\n",
    "init_preprocess = TimelinePreprocessing.PreprocessTimeline(dict_key_concept, keywords_to_match)\n",
    "\n",
    "#List of ids of timelines that were collected\n",
    "list_ids = list(db.test_timeline.distinct(\"user.id\"))\n",
    "\n",
    "# Loop through the list of user ids and preprocess each user's timeline\n",
    "for userid in tqdm(list_ids):\n",
    "    \n",
    "    # Load the user's timeline from the \"test_timeline\" collection\n",
    "    timeline = pd.DataFrame(db.test_timeline.find({\"user.id\": userid}))\n",
    "    \n",
    "    # Get the full text of each tweet in the timeline and add it to a new \"text\" column\n",
    "    timeline[\"text\"] = timeline.apply(lambda x: init_preprocess.get_full_text(x), axis=1)\n",
    "    \n",
    "    # Remove URLs, RTs, and mentions from the tweet text\n",
    "    timeline = init_preprocess.remove_url_rts_mentions(timeline)\n",
    "    \n",
    "    # Remove tweets that have less than 7 non-stopwords\n",
    "    timeline = init_preprocess.remove_empty_tweets(timeline, 7)\n",
    "    \n",
    "    # Translate non-English tweets to English\n",
    "    timeline = init_preprocess.format_date(timeline)\n",
    "\n",
    "    # Remove contractions \n",
    "    timeline[\"prep\"] = timeline.apply(lambda x: init_preprocess.remove_contractions(x), axis=1)\n",
    "\n",
    "    # Save timeline in new collection\n",
    "    db.test_prep.insert_many(timeline.to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different transformer models and keep the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Chose best transformer and test it on 1000 tweets to decide which one is the best\n",
    "df = pd.DataFrame(db.test_prep.find()[:1000])\n",
    "available_models = [\"all-mpnet-base-v2\"]#'stsb-mpnet-base-v2','stsb-roberta-base-v2','stsb-distilroberta-base-v2', 'nli-mpnet-base-v2', \n",
    "#'stsb-roberta-large', 'nli-roberta-base-v2', 'stsb-roberta-base', 'stsb-bert-large','stsb-distilbert-base', \n",
    "#'stsb-bert-base', 'nli-distilroberta-base-v2', 'paraphrase-xlm-r-multilingual-v1', 'paraphrase-distilroberta-base-v1', 'nli-bert-large', \n",
    "#'nli-distilbert-base', 'nli-roberta-large', 'nli-bert-large-max-pooling', 'nli-bert-large-cls-pooling', 'nli-distilbert-base-max-pooling', 'nli-roberta-base',\n",
    "# 'nli-bert-base-max-pooling', 'nli-bert-base', 'nli-bert-base-cls-pooling', 'average_word_embeddings_glove.6B.300d', 'average_word_embeddings_komninos', \n",
    "# 'average_word_embeddings_levy_dependency', 'average_word_embeddings_glove.840B.300d',\"sentence-t5-base\" #more can be added\n",
    "\n",
    "modeltest = ChooseTransformer.TestModel(\"dict_concepts_keywords.txt\")\n",
    "\n",
    "modeltest.try_models(df, available_models) #check values\n",
    "#Values shouldn't always be very high or very low. \n",
    "#Check if  related tweets have higher similarities for some concepts than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the different threshold from preprocessed tweets using choosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take 20000 tweets from the preprocessed timelines and apply similarities between tweets and the concepts\n",
    "#This will return the highest similarities and you will have to screen to decide when the score is enough or not\n",
    "threshold_df = pd.DataFrame(db.test_prep.find()[:20000])\n",
    "init_threshold = DefThreshold.DefineThreshold(threshold_df, \"all-mpnet-base-v2\")\n",
    "init_threshold.apply_similarities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End preprocessing by defining t0 for each user and deleting everything that was published before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t0 is the first time the user is tweeting about the disease/topic of interest\n",
    "#Here, it is the first time the user tweets about diabetes\n",
    "#For each timeline, we check when is the first time the threshold regarding diabetes is exceeded or the first time a diabetes \n",
    "#related keyword is used\n",
    "list_ids = list(db.test_prep.distinct(\"user.id\"))\n",
    "for userid in list_ids :\n",
    "    timeline = pd.DataFrame(db.test_prep.find({\"user.id\" : userid}))\n",
    "    timeline = init_preprocess.define_t0(timeline, 0.33, \"all-mpnet-base-v2\")\n",
    "    try:\n",
    "        db.test_prep_2.insert_many(timeline.to_dict('records'))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply similarities between all timelines and am exposure/outcome couple concepts and prepare Cohort-like table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Select an exposure and an outcome in the list of concepts\n",
    "exposure = \"Comorbidities\"\n",
    "outcome = \"Mental health\"\n",
    "#Load ids of remaining preprocessed timelines\n",
    "list_ids = db.test_prep_2.distinct(\"user.id\")\n",
    "#Apply similarities between tweets and an outcome and an exposure\n",
    "SimInit = TimelineSimilarities.Similarities(db, \"all-mpnet-base-v2\", list_ids)\n",
    "\n",
    "#Save similarities in new mongodb collection\n",
    "df_cohort = SimInit.apply_similarities(exposure, outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load ids and for each timeline check if the outcome and exposure previously filled appear\n",
    "list_ids = list(set(df_cohort.user.tolist()))\n",
    "init_cox = Data2Cox.PrepareCox(db, list_ids, CoxPHFitter)\n",
    "df_cohort_prep = init_cox.prepare_data(df_cohort)\n",
    "print(df_cohort_prep.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cox analysis and prints results\n",
    "init_cox.cox(df_cohort_prep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
